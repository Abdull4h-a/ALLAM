# -*- coding: utf-8 -*-
import logging
import pandas as pd
from telegram.helpers import escape_markdown
import torch
import torch.nn as nn
from transformers import AutoModel
from ibm_watsonx_ai.foundation_models import Model
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import Application, CommandHandler, MessageHandler, filters, CallbackQueryHandler, ContextTypes

# Set logging level to WARNING to suppress INFO logs
logging.basicConfig(level=logging.WARNING)

# Bot configuration
TELEGRAM_BOT_TOKEN = "7789650517:AAHbOf2Fl6vT3Atx2Kp-LyfNUnmWKbTAcdM"
ALLAM_API_KEY = "JsA-9hFRsH1msnDzUkcRpoUhSi3r3dfQMAj8Ic9yRiNg"
ALLAM_PROJECT_ID = "43ada3b1-5d11-42e8-9ef1-c718b99c05e1"
ALLAM_MODEL_ID = "sdaia/allam-1-13b-instruct"

# Load data and mappings
try:
    test_df = pd.read_csv('train_data.csv')
    author_to_idx_mapping = {name: i for i, name in enumerate(test_df['AuthorName'].sort_values().unique().tolist())}
    idx_to_author_mapping = {v: k for k, v in author_to_idx_mapping.items()}
except FileNotFoundError:
    logging.error("train_data.csv not found. Ensure the dataset is available.")
    test_df = pd.DataFrame()
    author_to_idx_mapping = {}
    idx_to_author_mapping = {}

# Set up device and models
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Define classifier model
class TextClassifier(nn.Module):
    def __init__(self, input_size=1024, hidden_size=512, num_classes=len(author_to_idx_mapping)):
        super(TextClassifier, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.ln1 = nn.LayerNorm(hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)
        self.ln2 = nn.LayerNorm(hidden_size // 2)
        self.fc3 = nn.Linear(hidden_size // 2, num_classes)

    def forward(self, x):
        x = self.fc1(x)
        x = self.ln1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.ln2(x)
        x = self.relu(x)
        x = self.fc3(x)
        x = torch.softmax(x, dim=-1)
        return x

# Initialize the embedding model
try:
    embedding_model = AutoModel.from_pretrained("jinaai/jina-embeddings-v3", trust_remote_code=True).to(device)
except Exception as e:
    logging.error(f"Error loading embedding model: {e}")
    embedding_model = None

# Load the after-training model (fine-tuned model) on the CPU
classification_head_after = TextClassifier(input_size=1024, hidden_size=1024)
try:
    classification_head_after.load_state_dict(
        torch.load('author_classifier_head_jina_embeddings.pt', map_location=torch.device('cpu'))
    )
    classification_head_after.to(device)
    classification_head_after.eval()
except FileNotFoundError:
    logging.error("Fine-tuned model file not found.")

# Initialize Allam API for rewriting
allam_model = Model(
    model_id=ALLAM_MODEL_ID,
    params={"max_new_tokens": 500},
    credentials={"url": "https://eu-de.ml.cloud.ibm.com", "apikey": ALLAM_API_KEY},
    project_id=ALLAM_PROJECT_ID
)

def rewrite_text_with_allam(author_name, text):
    """
    Rewrites the input text in the style of the specified author using the ALLAM model and PROMPT_TEMPLATE.

    Args:
    - author_name (str): The name of the author whose style should be applied.
    - text (str): The original text to rewrite.

    Returns:
    - str: The rewritten text or an error message if the process fails.
    """
    # Define the PROMPT_TEMPLATE
    PROMPT_TEMPLATE = f"""
    1. Ø£Ù†Øª ÙƒØ§ØªØ¨ Ø¹Ø±Ø¨ÙŠ Ø®Ø¨ÙŠØ±.
    2. Ø³ÙŠØªÙ… ØªØ²ÙˆÙŠØ¯Ùƒ Ø¨Ù†Øµ Ø¥Ø¯Ø®Ø§Ù„ ÙˆÙ†Øµ Ù…Ø±Ø¬Ø¹ÙŠ.
    3. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙØ¯Ø®Ù„ ÙˆØ¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨ØªÙ‡ Ù„ÙŠØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø£Ø³Ù„ÙˆØ¨ ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠ.
    4. ÙŠÙ…ÙƒÙ†Ùƒ ØªØºÙŠÙŠØ± Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª ÙˆØ§Ù„Ù‚ÙˆØ§Ø¹Ø¯ ÙˆÙ…Ø§ Ø¥Ù„Ù‰ Ø°Ù„Ùƒ Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù†Ù…Ø· ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ù†Ø§ØªØ¬ Ù…Ø¹ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØŒ ÙˆÙ„ÙƒÙ† Ù„Ø§ ÙŠÙ†Ø¨ØºÙŠ Ø£Ù† ÙŠØªØºÙŠØ± Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„ÙØ¹Ù„ÙŠ Ù„Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.
    5. Ù„Ø§ ØªØ®Ø±Ø¬ Ø£ÙŠ Ø´ÙŠØ¡ Ø¥Ø¶Ø§ÙÙŠ. ÙÙ‚Ø· Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¹Ø§Ø¯ ÙƒØªØ§Ø¨ØªÙ‡. Ù„Ø§ ØªÙ‚Ù… Ø¨Ø¥Ø®Ø±Ø§Ø¬ ÙˆØµÙ Ø§Ù„Ù†Øµ.

    <Example_1>
        <Ù…Ø±Ø¬Ø¹ÙŠ>
        Ø£ÙŠÙ‡Ù…Ø§ Ø£ÙƒØ¨Ø±: Ø§Ù„ÙƒÙˆÙ† Ø£Ù… Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„Ø¥Ù†Ø³Ø§Ù†ÙŠØ©ØŸ Ø¥Ù† Ø§Ù„Ø­ÙŠØ§Ø© Ø¥Ù† Ù„Ù… ØªÙƒÙ† Ù„Ù‡Ø§ ØºØ§ÙŠØ© Ø¨Ø¹ÙŠØ¯Ø© Ù…ÙˆØµÙˆÙ„Ø© Ø¨Ø§Ù„ØºØ§ÙŠØ© Ø§Ù„ØªÙŠ ÙŠØ³Ø¹Ù‰ Ø¥Ù„ÙŠÙ‡Ø§ Ø§Ù„ÙƒÙˆÙ† Ø¨Ø±Ù…ØªÙ‡ ÙÙ‡ÙŠ ÙˆÙ„Ø§ Ø±ÙŠØ¨ Ø£ØµØºØ± Ù…Ù† Ø£Ù† ØªÙ‚Ø§Ø³ Ø¥Ù„ÙŠÙ‡ØŒ Ø£Ùˆ ÙŠÙØ§Ø¶Ù„ Ø¨ÙŠÙ†Ù‡Ø§ ÙˆØ¨ÙŠÙ†Ù‡. ÙˆÙ‚Ø¯ ÙƒØ§Ù† ÙŠÙƒÙÙŠÙ†Ø§ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„ÙØ±Ø¶ ÙƒØ±ØªÙ†Ø§ Ø§Ù„Ø£Ø±Ø¶ÙŠØ© ÙˆØ­Ø¯Ù‡Ø§ Ø£Ùˆ Ù†Ø¸Ø§Ù… ÙˆØ§Ø­Ø¯ Ù…Ù† Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø´Ù…ÙˆØ³ Ø§Ù„ØªÙŠ Ù„Ø§ Ø¹Ø¯Ø§Ø¯ Ù„Ù‡Ø§. ÙˆØ¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„Ø¥Ù†Ø³Ø§Ù†ÙŠØ© Ù‡ÙŠ Ø§Ù„Ø­Ø³ Ø§Ù„Ø´Ø§Ø¹Ø± Ø§Ù„Ù…ÙØ±Ø¯ ÙÙŠ Ø§Ù„ÙˆØ¬ÙˆØ¯ØŒ ÙÙ„ÙÙ…Ù Ù„Ù… ÙŠÙƒÙ† Ù„Ù‡Ø§ Ù…Ù† Ø§Ù„Ø¥Ø­Ø³Ø§Ø³ Ø§Ù„Ù‚Ø¯Ø± Ø§Ù„ÙƒØ§ÙÙŠ Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„ÙˆØ¬ÙˆØ¯ Ø­Ù‚ Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŸ ÙˆÙ„ÙÙ…Ù Ù„ÙÙ…Ù’ ÙŠØªÙ†Ø§Ø³Ø¨ Ø§Ù„Ø¹Ø§Ø±Ù ÙˆØ§Ù„Ù…Ø¹Ø±ÙˆÙ Ø£Ùˆ ÙŠØªÙ‚Ø§Ø±Ø¨Ø§ØŸ Ø£Ù„Ø§ Ù†ÙÙ‡Ù… Ù…Ù† Ø°Ù„Ùƒ Ø£Ù†Ù‡ Ù„Ø§ Ø¨Ø¯ ÙÙŠ Ø§Ù„ÙˆØ¬ÙˆØ¯ Ù…Ù† Ù‚Ø¯Ø±Ø© ØªØ¹Ø±ÙÙ‡ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø®Ù„ÙŠÙ‚Ø© Ø¨Ù‡ØŸ Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ø®Ø§Ø·Ø± Ø§Ù„Ø°ÙŠ Ù‚Ø§Ù… Ø¨Ù†ÙØ³ÙŠ Ø¹Ù†Ø¯ Ù†Ø¸Ù… Ø§Ù„Ø£Ø¨ÙŠØ§Øª Ø§Ù„Ø¢ØªÙŠØ©.
        </Ù…Ø±Ø¬Ø¹ÙŠ>

        <Ø¥Ø¯Ø®Ø§Ù„>
        Ù„ÙÙ…Ù Ù‚Ø¨ÙØ­ Ø§Ù„Ø«Ù†Ø§Ø¡ ÙÙŠ Ø§Ù„ÙˆØ¬Ù‡ Ø­ØªÙ‰ ØªÙˆØ§Ø·Ø¦ÙˆØ§ Ø¹Ù„Ù‰ ØªØ²ÙŠÙŠÙÙ‡ØŸ ÙˆÙ„ÙÙ…Ù Ø­Ø³ÙÙ† ÙÙŠ Ø§Ù„Ù…ØºÙŠØ¨ Ø­ØªÙ‰ ØªÙÙ…ÙÙ†Ù‘ÙÙŠÙ Ø°Ù„Ùƒ Ø¨ÙƒÙ„ Ù…Ø¹Ù†Ù‹Ù‰ØŸ Ø£Ù„Ø£Ù† Ø§Ù„Ø«Ù†Ø§Ø¡ ÙÙŠ Ø§Ù„ÙˆØ¬Ù‡ Ø£Ø´Ø¨Ù‡ Ø§Ù„Ù…Ù„Ù‚ ÙˆØ§Ù„Ø®Ø¯ÙŠØ¹Ø© ÙˆÙÙŠ Ø§Ù„Ù…ØºÙŠØ¨ Ø£Ø´Ø¨Ù‡ Ø§Ù„Ø¥Ø®Ù„Ø§Øµ ÙˆØ§Ù„ØªÙ‘ÙÙƒÙ’Ø±ÙÙ…ÙØ© Ø£Ù… Ù„ØºÙŠØ± Ø°Ù„ÙƒØŸ Ù‚Ø§Ù„ Ø£Ø¨Ùˆ Ø¹Ù„ÙŠ Ù…Ø³ÙƒÙˆÙŠÙ‡ØŒ Ø±Ø­Ù…Ù‡ Ø§Ù„Ù„Ù‡: Ù„Ù…Ø§ ÙƒØ§Ù† Ø§Ù„Ø«Ù†Ø§Ø¡ ÙÙŠ Ø§Ù„ÙˆØ¬Ù‡ Ø¹Ù„Ù‰ Ø§Ù„Ø£ÙƒØ«Ø± Ø¥Ø¹Ø§Ø±Ø© Ø´Ù‡Ø§Ø¯Ø©Ù Ø¨ÙØ¶Ø§Ø¦Ù„ Ø§Ù„Ù†ÙØ³ ÙˆØ®Ø¯ÙŠØ¹Ø© Ø§Ù„Ø¥Ù†Ø³Ø§Ù† Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ø´Ù‡Ø§Ø¯Ø© Ø­ØªÙ‰ ØµØ§Ø± Ø°Ù„Ùƒ â€” Ù„Ø§ØºØªØ±Ø§Ø±Ù‡ ÙˆØªØ±ÙƒÙ‡ ÙƒØ«ÙŠØ±Ù‹Ø§ Ù…Ù† Ø§Ù„Ø§Ø¬ØªÙ‡Ø§Ø¯ ÙÙŠ ØªØ­ØµÙŠÙ„ Ø§Ù„ÙØ¶Ø§Ø¦Ù„ØŒ ÙˆØºØ±Ø¶ ÙØ§Ø¹Ù„Ù Ø°Ù„Ùƒ Ø§Ø­ØªØ±Ø§Ø² Ù…ÙˆØ¯Ø© ØµØ§Ø­Ø¨Ù‡ Ø¥Ù„Ù‰.
        </Ø¥Ø¯Ø®Ø§Ù„>

        <Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª>
        Ù„ÙÙ…Ù ÙƒØ§Ù† Ø§Ù„Ø«Ù†Ø§Ø¡ ÙÙŠ Ø§Ù„Ø­Ø¶ÙˆØ± Ù…ÙƒØ±ÙˆÙ‡Ù‹Ø§ Ø­ØªÙ‰ Ø£ÙØ¬Ù’Ù…ÙØ¹Ù Ø§Ù„Ù†Ø§Ø³ Ø¹Ù„Ù‰ Ø²ÙŠÙÙ‡ØŸ ÙˆÙ„ÙÙ…Ù ÙƒØ§Ù† Ø§Ù„Ø«Ù†Ø§Ø¡ ÙÙŠ Ø§Ù„ØºÙŠØ§Ø¨ Ù…Ø­Ù…ÙˆØ¯Ù‹Ø§ Ø­ØªÙ‰ ØªÙØ­ÙØ¨Ù‘ÙØ¨ÙØªÙ’ Ø¥Ù„ÙŠÙ‡ Ø§Ù„Ù†ÙÙˆØ³ ÙÙŠ ÙƒÙ„ Ù…Ù‚Ø§Ù…ØŸ Ø£ÙÙ„ÙØ£ÙÙ†Ù‘Ù Ø§Ù„Ø«Ù†Ø§Ø¡ ÙÙŠ Ø§Ù„Ø­Ø¶ÙˆØ± Ø£Ù‚Ø±Ø¨ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø§Ù‡Ù†Ø© ÙˆØ§Ù„Ø®Ø¯Ø§Ø¹ØŒ ÙˆÙÙŠ Ø§Ù„ØºÙŠØ§Ø¨ Ø£Ù‚Ø±Ø¨ Ø¥Ù„Ù‰ Ø§Ù„Ø¥Ø®Ù„Ø§Øµ ÙˆØ§Ù„ØªÙƒØ±ÙŠÙ…ØŸ Ø£Ù… Ø£Ù†Ù‘Ù ÙÙŠ Ø§Ù„Ø£Ù…Ø± Ù…Ø¹Ù†Ù‰ Ø¢Ø®Ø±ØŸ ÙÙ‚Ø¯ Ù‚Ø§Ù„ Ø£Ø¨Ùˆ Ø¹Ù„ÙŠ Ù…Ø³ÙƒÙˆÙŠÙ‡ØŒ Ø±Ø­Ù…Ù‡ Ø§Ù„Ù„Ù‡: Ø¥Ø° ÙƒØ§Ù† Ø§Ù„Ø«Ù†Ø§Ø¡ ÙÙŠ Ø§Ù„Ø­Ø¶ÙˆØ± ÙÙŠ Ø§Ù„ØºØ§Ù„Ø¨ Ø´Ù‡Ø§Ø¯Ø© Ù…ÙØ¹Ø§Ø±ÙØ© Ø¹Ù„Ù‰ ÙØ¶Ø§Ø¦Ù„ Ø§Ù„Ù†ÙØ³ØŒ ÙˆØ®Ø¯ÙŠØ¹Ø©Ù‹ Ù„Ù„Ù†ÙØ³ Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ø´Ù‡Ø§Ø¯Ø©ØŒ Ø­ØªÙ‰ ØµØ§Ø± Ø°Ù„Ùƒ Ø³Ø¨Ø¨Ù‹Ø§ Ù„Ø§Ù†Ø®Ø¯Ø§Ø¹ Ø§Ù„Ù…Ø±Ø¡ ÙˆØªØ±ÙƒÙ‡ Ø§Ù„Ø³Ø¹ÙŠ Ø§Ù„Ø¬Ø§Ø¯ Ù„ØªØ­ØµÙŠÙ„ Ø§Ù„ÙØ¶Ø§Ø¦Ù„ØŒ ÙˆØºØ±Ø¶ Ø§Ù„Ù…Ø§Ø¯Ø­ Ø¨Ø°Ù„Ùƒ Ø¥Ø±Ø¶Ø§Ø¡ ØµØ¯ÙŠÙ‚Ù‡ ÙˆØ§Ù„ØªÙˆØ¯Ø¯ Ø¥Ù„ÙŠÙ‡.
        </Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª>
    </Example_1>

    <Example_2>
        <Ù…Ø±Ø¬Ø¹ÙŠ>
        {author_name}
        </Ù…Ø±Ø¬Ø¹ÙŠ>

        <Ø¥Ø¯Ø®Ø§Ù„>
        {text}
        </Ø¥Ø¯Ø®Ø§Ù„>

        <Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª>
    """

    try:
        # Generate the rewritten text using the ALLAM model
        response = allam_model.generate_text(PROMPT_TEMPLATE)
        return response.strip()
    except Exception as e:
        logging.error(f"Error in Allam API: {e}")
        return "Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù†Øµ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù„Ø§Ø­Ù‚Ù‹Ø§."


def classify_with_fine_tuned_model(text):
    """
    Classifies the author of a long text, ensuring a single prediction for the whole input.

    Args:
    - text (str): The input text.

    Returns:
    - str: The predicted author name or "Unknown".
    """
    if embedding_model is None:
        return "Unknown"
    with torch.no_grad():
        # Split the text into smaller chunks if it exceeds max length
        max_chunk_size = 2048
        chunks = [text[i:i+max_chunk_size] for i in range(0, len(text), max_chunk_size)]
        
        # Generate embeddings for all chunks
        embeddings = []
        for chunk in chunks:
            chunk_embedding = embedding_model.encode([chunk], max_length=max_chunk_size)
            embeddings.append(torch.from_numpy(chunk_embedding).to(device))
        
        # Average embeddings to create a single representation
        combined_embedding = torch.mean(torch.stack(embeddings), dim=0)

        # Perform classification on the combined embedding
        logits = classification_head_after(combined_embedding)
        predicted_idx = torch.argmax(logits, -1).item()
        return idx_to_author_mapping.get(predicted_idx, "Unknown")


async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    keyboard = [
        [InlineKeyboardButton("ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Øµ", callback_data='classify')],
        [InlineKeyboardButton("Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù†Øµ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø¢Ø®Ø±", callback_data='rewrite')],
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)

    # Define the image path or URL
    image_path = "image.jpg"  # Replace with the path to your uploaded image
    # Or use a URL if hosting the image online:
    # image_url = "https://example.com/image.png"

    # Send the image
    try:
        await context.bot.send_photo(
            chat_id=update.effective_chat.id,
            photo=open(image_path, "rb"),  # Use `image_url` if the image is hosted online
            caption=(
                "ğŸ‘‹ **Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ!** Ø§Ø®ØªØ± Ø£Ø­Ø¯ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø£Ø¯Ù†Ø§Ù‡:\n\n"
                "- ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Øµ Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¤Ù„Ù Ø§Ù„Ù…Ø­ØªÙ…Ù„.\n"
                "- Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù†Øµ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù…Ø¤Ù„Ù Ù…Ø­Ø¯Ø¯."
            ),
            parse_mode='Markdown',
            reply_markup=reply_markup
        )
    except FileNotFoundError:
        # Fallback if the image is missing
        await update.message.reply_text(
            "âš ï¸ ØªØ¹Ø°Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„ØµÙˆØ±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ØµØ­ÙŠØ­."
        )
    except Exception as e:
        # Handle other errors
        await update.message.reply_text(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØµÙˆØ±Ø©: {str(e)}")



async def button_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()

    if query.data == 'classify':
        await query.edit_message_text("ğŸ“ ÙŠØ±Ø¬Ù‰ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ ØªØµÙ†ÙŠÙÙ‡ Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¤Ù„Ù Ø§Ù„Ù…Ø­ØªÙ…Ù„.")
        context.user_data['action'] = 'classify'
    elif query.data == 'rewrite':
        await query.edit_message_text(
            "âœï¸ **Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù†Øµ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù…Ø¤Ù„Ù:**\n\n"
            "1ï¸âƒ£ Ø£Ø±Ø³Ù„ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨ØªÙ‡.\n"
            "2ï¸âƒ£ Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ø³Ø£Ø·Ù„Ø¨ Ù…Ù†Ùƒ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ø³Ù… Ø§Ù„Ù…Ø¤Ù„Ù Ø¨Ø£Ø³Ù„ÙˆØ¨Ù‡ Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ÙƒØªØ§Ø¨Ø©."
        )
        context.user_data['action'] = 'rewrite'
        context.user_data['rewrite_step'] = 1


def split_text(text, max_length=4000):
    """
    Splits a long text into smaller chunks, each with a maximum length.

    Args:
    - text (str): The text to be split.
    - max_length (int): The maximum length of each chunk.

    Returns:
    - List[str]: A list of text chunks.
    """
    chunks = []
    while len(text) > max_length:
        # Find the last space within the limit to avoid breaking words
        split_at = text.rfind(' ', 0, max_length)
        if split_at == -1:  # No space found, split at max_length
            split_at = max_length
        chunks.append(text[:split_at])
        text = text[split_at:].strip()
    chunks.append(text)  # Add the remaining text
    return chunks

async def handle_user_text(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_text = update.message.text
    action = context.user_data.get('action')

    if action == 'classify':
        # Step 1: Classify the author for the entire text
        predicted_author = classify_with_fine_tuned_model(user_text)
        response_text = f"ğŸ” **ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø¤Ù„Ù**: Ø§Ù„Ù†Øµ ÙŠØ´Ø¨Ù‡ Ø¥Ù„Ù‰ Ø­Ø¯ ÙƒØ¨ÙŠØ± Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ù…Ø¤Ù„Ù: **{predicted_author}**."
        await update.message.reply_text(response_text, parse_mode='Markdown')

    elif action == 'rewrite':
        rewrite_step = context.user_data.get('rewrite_step', 1)

        if rewrite_step == 1:
            # Save the original text and prompt for the author's name
            context.user_data['original_text'] = user_text
            await update.message.reply_text("ğŸ–‹ **Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø«Ø§Ù†ÙŠØ©:**\nÙŠØ±Ø¬Ù‰ Ø§Ù„Ø¢Ù† Ø¥Ø¯Ø®Ø§Ù„ Ø§Ø³Ù… Ø§Ù„Ù…Ø¤Ù„Ù Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù†Øµ Ø¨Ø£Ø³Ù„ÙˆØ¨Ù‡.")
            context.user_data['rewrite_step'] = 2

        elif rewrite_step == 2:
            # Perform the rewrite based on the original text and provided author name
            original_text = context.user_data.get('original_text')
            author_name = user_text
            rewritten_text = rewrite_text_with_allam(author_name.strip(), original_text.strip())

            # Define the image path or URL
            image_path = "image.jpg"  # Local image path
            # Or use an online image URL: image_url = "https://example.com/image.jpg"

            # Escape special characters in MarkdownV2
            response_text = (
                f"ğŸ“ **Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙØ¹Ø§Ø¯ ÙƒØªØ§Ø¨ØªÙ‡ Ø¨Ø£Ø³Ù„ÙˆØ¨ {escape_markdown(author_name.strip(), version=2)}:**\n\n"
                f"{escape_markdown(rewritten_text, version=2)}"
            )

            try:
                # Send the image with the caption
                await context.bot.send_photo(
                    chat_id=update.effective_chat.id,
                    photo=open(image_path, "rb"),  # Replace with `photo=image_url` for online images
                    caption=response_text,
                    parse_mode='MarkdownV2',
                )
            except FileNotFoundError:
                # Handle missing image file
                await update.message.reply_text("âš  Ø¹Ø°Ø±Ù‹Ø§ØŒ ØªØ¹Ø°Ø± Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©.")
            except Exception as e:
                # Handle other errors
                await update.message.reply_text(f"âš  Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØµÙˆØ±Ø©: {str(e)}")

            # Reset the rewrite step
            context.user_data['rewrite_step'] = 1

        else:
            await update.message.reply_text("âš  ÙŠØ±Ø¬Ù‰ Ø§Ø®ØªÙŠØ§Ø± Ø¥Ø¬Ø±Ø§Ø¡ Ù‚Ø¨Ù„ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù†Øµ.")



# Main bot setup
def main():
    application = Application.builder().token(TELEGRAM_BOT_TOKEN).build()

    application.add_handler(CommandHandler("start", start))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_user_text))
    application.add_handler(CallbackQueryHandler(button_handler))

    application.run_polling()


if __name__ == '__main__':
    main()
